<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="icon" href="images/me.jpg" type="image/jpeg">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Ananya Jana | Research</title>

  <!-- Styles -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" />
  <link href="css/style.css" rel="stylesheet" type="text/css" />
  <style>
body {
  background-color: #f9f9f9;
  color: #333;
  font-family: 'Lato', sans-serif;  /* <-- new modern font */
  font-size: 15px;
  line-height: 1.6;
  padding: 0;
  margin: 0;
}

  .tab-container {
    display: flex;
    justify-content: center;
    background: white;
    border-bottom: 2px solid #ccc;
  }

  .tab {
    padding: 10px 20px;
    text-decoration: none;
    color: #004080;
    font-weight: bold;
    border: none;
    border-bottom: 3px solid transparent;
  }

  .tab:hover {
    background-color: #f1f1f1;
  }

  .tab.active {
    border-bottom: 3px solid #004080;
    background-color: #e0e0e0;
  }

  .container {
    max-width: 960px;
    margin: 2em auto;
    padding: 1em;
    background: white;
    font-size: 15px; /* Match font size */
  }

  h1, h2 {
    color: #004080;
  }

  a {
    color: #004080;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  ul li {
    font-size: 15px; /* Match font size */
  }

  img {
    max-width: 100%;
    border-radius: 8px;
  }
</style>
</head>

<body>

  <!-- Navigation Tabs -->
  <div class="tab-container">
    <a href="index.html" class="tab">Home</a>
    <a href="dr-ananya-jana.html" class="tab">Dr. Ananya Jana</a>
    <a href="teaching.html" class="tab">Teaching</a>
    <a href="research.html" class="tab active">Research</a>
    <a href="grants.html" class="tab">Grants</a>
    <a href="publication.html" class="tab">Publication</a>
    <a href="opening.html" class="tab">Opening</a>
    <a href="people.html" class="tab">People</a>
  </div>

  <!-- Main Content -->
  <div class="container">
    <h2>Research</h2>
    <p>
      The CVAIR Lab conducts research in computer vision, deep learning, and artificial intelligence.
      We aim to develop impactful AI solutions for real-world problems.
    </p>

    <h3>Ongoing Projects</h3>
    
    <h4>üßç‚Äç‚ôÄÔ∏è A Vision Transformer and Diffusion-Based Hybrid AI Framework for 3D Human Pose Estimation</h4>
    <p>
      This project introduces a hybrid AI framework combining Vision Transformers and Diffusion Models for accurate 3D human pose estimation from 2D images. The transformer captures global spatial features, while the diffusion model refines pose predictions through probabilistic reasoning, improving robustness in complex scenes.
    </p>

    <h4>ü¶∑ Tooth Landmark Localization Using Deep Learning</h4>
    <p>
      This project focuses on accurately identifying anatomical landmarks on 3D dental scans using convolutional neural networks (CNNs).
      Automated tooth landmark localization is a critical task in orthodontic planning and forensic identification.
      Our approach improves precision and consistency in landmark detection, reducing reliance on manual annotations and enabling large-scale dental morphometric analysis.
    </p>

    <h4>üßç‚Äç‚ôÄÔ∏è Exploring Deep Learning-based Human Mesh Recovery from 2D Images with Textual Description</h4>
    <p>
      This project focuses on reconstructing 3D human mesh representations from single 2D images, guided by textual prompts.
      By integrating natural language processing with vision-based models, this project aims to enhance mesh recovery for personalized healthcare, rehabilitation, and human-computer interaction.
    </p>

    <h4>üß¨ Using Generative AI for Biomedical Image Synthesis</h4>
    <p>
      This project explores the use of generative adversarial networks (GANs) and diffusion models to synthesize high-quality biomedical images.
      Synthetic image generation is crucial for augmenting training datasets in domains where data is limited or annotations are costly.
      Our research emphasizes fidelity, clinical realism, and diversity in generated outputs, supporting applications in training, anomaly detection, and privacy-preserving AI.
    </p>

    <h4>üß† Exploration of Vision-Language Models (VLMs) and Large Language Models (LLMs) for Colorectal Cancer Segmentation</h4>
    <p>
      We explore the integration of multimodal AI models ‚Äî particularly VLMs and LLMs ‚Äî for semantic segmentation of colorectal cancer from histopathology and radiology images.
      This research aims to bridge textual medical knowledge with visual analysis, enabling explainable and robust segmentation under low-data conditions.
      By leveraging models like CLIP, Segment Anything, and GPT-4, the project pushes toward zero-shot or few-shot learning in medical imaging.
    </p>
  </div>

</body>
</html>

